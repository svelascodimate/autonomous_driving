{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Github Miner \n",
    "> Module to extract issues from github open source repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp miner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import requests\n",
    "import json\n",
    "from urllib.parse import urlencode\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def search_github_issues(query, headers, GITHUB_API_URL, page=1, per_page=10):\n",
    "    \"\"\"Search issues across repositories on GitHub.\"\"\"\n",
    "    encoded_query = urlencode({\"q\": query})\n",
    "    url = f\"{GITHUB_API_URL}?{encoded_query}&page={page}&per_page={per_page}\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        raise Exception(f\"Error: {response.status_code}, {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def extract_issue_data(issues):\n",
    "    \"\"\"Extract relevant data from issues.\"\"\"\n",
    "    extracted_data = []\n",
    "    for issue in issues:\n",
    "        issue_data = {\n",
    "            \"id\": issue['id'],\n",
    "            \"state\" : issue['state'],\n",
    "            \"title\": issue['title'],\n",
    "            \"url\": issue['html_url'],\n",
    "            \"body\": issue['body'],\n",
    "            \"repository_url\": issue['repository_url'],\n",
    "            \"pull_request\" : issue['pull_request']['url'] if 'pull_request' in  issue else None,\n",
    "            \"comments_url\" : issue['comments_url'],\n",
    "            \"labels\" : [{'name': label['name'], 'description': label['description']} for label in issue['labels']]\n",
    "        }\n",
    "        extracted_data.append(issue_data)\n",
    "    return extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def date_range(start_date, end_date, delta=timedelta(days=365)):\n",
    "    \"\"\"Generate date ranges.\"\"\"\n",
    "    current_date = start_date\n",
    "    while current_date < end_date:\n",
    "        yield current_date, min(current_date + delta, end_date)\n",
    "        current_date += delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def save_data_to_json(data, filename):\n",
    "    \"\"\"Save data to a JSON file.\"\"\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def load_data_from_json(filename):\n",
    "    \"\"\"Load data from a JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            return json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        return []  # Return an empty list if the file does not exist\n",
    "    except json.JSONDecodeError:\n",
    "        return []  # Return an empty list if the file is empty or has invalid JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def append_new_data_to_json(new_data, filename):\n",
    "    \"\"\"Append new data to an existing JSON file.\"\"\"\n",
    "    existing_data = load_data_from_json(filename)\n",
    "    existing_data.extend(new_data)\n",
    "    save_data_to_json(existing_data, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def fetch_data_and_get_results_by_count(query: str, HEADERS, GITHUB_API_URL):\n",
    "    \"\"\"fetch issues matching the query and returns the max possible number of issues\"\"\"\n",
    "    PER_PAGE = 100  # Max is 100\n",
    "    all_issues = []\n",
    "    current_page = 1\n",
    "    total_count = None\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            results = search_github_issues(query, HEADERS, GITHUB_API_URL, page=current_page, per_page=PER_PAGE)\n",
    "            if total_count is None:\n",
    "                total_count = results['total_count']\n",
    "                print(f\"Total issues to fetch: {total_count}\")\n",
    "            issues = results.get(\"items\", [])\n",
    "            if not issues:\n",
    "                break\n",
    "            all_issues.extend(extract_issue_data(issues))\n",
    "            print(f\"Page {current_page}: {len(issues)} issues fetched\")\n",
    "            if len(all_issues) >= total_count or len(issues) < PER_PAGE:\n",
    "                break\n",
    "            current_page += 1\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            break\n",
    "\n",
    "\n",
    "    print(f\"Total issues fetched: {len(all_issues)}\")\n",
    "    return all_issues\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
